[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Scrolltelling",
    "section": "",
    "text": "Scrollytelling stories let you explain complicated concepts to readers as they scroll down the page. You could build up a complicated plot layer-by-layer, zoom in on a famous map, highlight a key quote from an interviewee, or even animate your own web graphics.\nThis is my first financial report presented in a scrolltelling."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Financial Risk Analysis",
    "section": "",
    "text": "Financial Risk Analysis using the OGARCH model   by Erika Atoma December, 2024 \n\n\n\n\nThis is a financial analysis of the asset volatility and volatility models However, this is not financial advice for people to invest in the semiconductor manufacturing stocks.\n\n\n\n\nI am giving credit to the following books for equipping me with this knowledge on Financial Risk Forecasting:\n\n\n\n\nThe Theory and Practice of Forecasting Market Risk with Implementation in R and Matlab by Jon Danielsson\n\n\n\n\n& An Introduction to Analysis of Financial Data with R by Ruey S. Tsay.\n\n\n\n\n The first thing to consider is, What is Volatility? \n\n\n\n\nAs a trader, the degree of variations in a trading price series over time.\n\n\n\n\nAlthough asset volatility is well defined, it is not directly observable in practice. What we observe are the prices of an asset and its derivatives. One must estimate the volatility from these observed prices. The fact that volatility is not directly observable has several important implications in studying and modeling volatility.\n\n\n\n\nThere are the uni-variate models, including the auto-regressive conditional heteroscedastic (ARCH) model of Engle (1982)\n\n\n\n\nand the generalized auto-regressive conditional heteroscedastic (GARCH) model of Bollerslev (1986).\n\n\n\n\nBut, I will focus on the multivariate model – OGARCH\nThe Orthogonal GARCH (O-GARCH) or principal components GARCH method adopts a somewhat different approach. The principal components approach has first been applied in a GARCHtype context by Ding (1994). Shortly after, Alexander and Chibumba (1996) introduced the strongly related O-GARCH model.\nThereafter, O-GARCH has been a popular choice to model the conditional covariances of financial data (see e.g. Klaassen, 1999), mainly because the model remains feasible for large covariances matrices (see e.g. Alexander, 2002).\nRecently, the model has been elaborated along with applications by Alexander (1998, 2001).\n\n\n\n\nThis study examines six stocks which are:\nQualcomm (QCOM), Broadcom Inc. (AVGO), Micron Technology, Inc. (MU), Texas Instruments Incorporated (TXN), Advanced Micro Devices, Inc. (AMD), and NVIDIA Corporation (NVDA).\nThe data was downloaded from the Yahoo finance website, taking the adjusted prices of these stocks. The sample period of the data was chosen at a starting date from 1 January 2014 to an ending date of 31 December 2023. This sample period was selected as it consists of extensive data, increasing the research’s validity.\nThe daily prices exhibit certain degrees of variability and show an upward movement during the sample period.\n\n\n\n\nThe daily adjusted prices were converted to daily log returns because it becomes easier to handle to gain a full-scale statistical model summary of the portfolio investment. Furthermore, the foundation of statistical inference in time series analysis is the concept of weak stationary.\n\n\n\n\nThe monthly log returns of QCOM vary around 0 over time. It shows that the range of monthly log returns is approximately [-0.2, 0.2] through the sample span. We can predict a reasonable confidence that the future monthly returns will be around 0 and vary between -0.2 and 0.2.\nThe monthly log returns of AVGO vary around 0 too, over time. It shows that the range of the monthly log returns is approximately [-0.2, 0.1]. Therefore, we can predict reasonable confidence that the future monthly returns will be around 0 and vary between -0.2 and 0.1.\n\n\n\n\nFor AMD, it shows some extreme log return range between 2016 and 2018. Apart from that, the range of the monthly log returns is approximately [-0.2, 0.2]. Therefore, we can predict reasonable confidence that the future monthly returns will be around 0 and vary between -0.2 and 0.2.\n\n\n\n\nWhat happened around 2016-2018 for AMD stocks? \n\n\n\n\nIntroduction of the Ryzen Processors: AMD was struggling financially in the early 2010s but started to regain market confidence in 2016 with the development and preview of its Ryzen CPU lineup (released in 2017). These chips were poised to challenge Intel’s dominance in the CPU market. Moammer, K. (2017, July 4). AMD takes 10.4% CPU share from Intel in Q2 2017 in Passmark Survey, its largest quarterly share gain in history. Wccftech.\n\n\n\n\nFor MU, the results and the prediction are similar to AVGO.\n\n\n\n\nFor TXN, the monthly log returns of TXN vary around 0. It shows that the range of monthly log returns is approximately [-0.1, 0.1]. Therefore, we can predict reasonable confidence that the future monthly returns will be around 0 and vary between -0.1 and 0.1.\nFor NVDA, the range of the monthly log returns is approximately [-0.1, 0.2], apart from the extreme trading days around 2016-2018, 2020, 2023. Consequently, we can predict reasonable confidence that the future monthly returns will be around 0 and vary between -0.1 and 0.2.\n\n\n\n\n What happened between 2016-2018 for NVDA stocks? \n\n\n\n\nAI and Deep Learning Revolution: NVIDIA capitalized on the emerging demand for GPUs in AI and deep learning applications. The launch of the Pascal GPU architecture (e.g., GTX 10 series) in 2016 positioned NVIDIA as the leader in this space.\nZacks Equity Research. (2017, July 4). NVIDIA(NVDA) Unveils GeForce GTX 10-Series for Notebooks. YahooFinance\n\n\n\n\n Caution!!! While these plots offer valuable insights, it’s important to remember that past performance is not indicative of future results. Market conditions can change rapidly, and unforeseen events can significantly impact stock prices. Additionally, it’s crucial to consider other factors, such as economic indicators, industry trends, and company-specific news, when making investment decisions. \n\n\n\n\n Table (Summary Statistics) \nAvgRet (Average Return):\n\nDefinition: The mean return over the observation period.\nNVIDIA has the highest AvgRet (0.0019), indicating the best average performance.\nMicron Technology has the lowest AvgRet (0.0006), reflecting relatively weaker average performance.\n\nStdDevRet (Standard Deviation of Returns):\n\nDefinition: A measure of the variability (volatility) of returns.\nAdvanced Micro Devices (AMD) has the highest StdDevRet (0.0360), meaning it’s the most volatile.\nTexas Instruments has the lowest StdDevRet (0.0174), suggesting it’s the least volatile.\n\nMaxRet (Maximum Return):\n\nDefinition: The highest single-day (or period) return.\nAMD has the highest MaxRet (0.4206), showing it experienced a large positive return at least once.\nMicron Technology has the lowest MaxRet (0.1252), suggesting fewer extreme positive movements.\n\nMinRet (Minimum Return):\n\nDefinition: The lowest single-day (or period) return.\nAMD has the lowest MinRet (-0.2775), reflecting significant potential downside risk.\nTexas Instruments has the highest MinRet (-0.1259), suggesting smaller negative extremes.\n\nSkewRet (Skewness of Returns):\n\nDefinition: A measure of the asymmetry in the return distribution.\nAMD has the highest skew (0.4762), indicating a bias toward more extreme positive returns.\nBroadcom has the most negative skew (-0.4530), suggesting a higher likelihood of extreme negative returns.\n\nKurtRet (Kurtosis of Returns):\n\nDefinition: A measure of the “tailedness” or outlier potential of the return distribution.\nAMD and Qualcomm have high kurtosis values (10.9135 and 10.1084, respectively), meaning their returns have a higher chance of extreme movements.\nMicron Technology has the lowest kurtosis (3.7774), suggesting more stable returns.\n\nFinal results:\n\nQualcomm: Low AvgRet, moderate volatility, and high kurtosis (prone to outliers).\nBroadcom: Modest AvgRet but negatively skewed, indicating risk of extreme losses.\nAMD: High AvgRet, highest volatility, positive skew, and high kurtosis (a high-risk, high-reward profile).\nMicron Technology: Low AvgRet, moderate volatility, and the least kurtosis (most stable).\nTexas Instruments: Low AvgRet, low volatility, and relatively balanced skew and kurtosis (low-risk profile).\nNVIDIA: Highest AvgRet, moderate volatility, positive skew, and high kurtosis (potential for strong performance but prone to outliers).\n\n\n\n\n\n Another Statistical methods: Correlation Matrix \nDiagonal Values (1.0000):\n\nThe diagonal entries represent the correlation of each stock with itself, which is always 1 (perfect correlation).\n\nHigh Correlations (0.6–0.7):\n\nTexas Instruments and Broadcom (0.6941): These two stocks have the strongest correlation in the matrix, suggesting their returns tend to move together closely.\nTexas Instruments and Qualcomm (0.6093): These two stocks are also highly correlated.\n\nModerate Correlations (0.4–0.6):\n\nNVIDIA and Texas Instruments (0.6340): Moderate positive correlation, likely reflecting shared exposure to the semiconductor and technology sectors.\nNVIDIA and AMD (0.5899): As competitors in the GPU and AI chip market, their returns show moderate correlation due to shared market drivers.\n\nLower Correlations (0.4–0.5):\n\nAMD and Qualcomm (0.4256): This relatively lower correlation indicates their returns are less strongly related, possibly due to differences in their market segments (e.g., CPUs vs. mobile technologies).\n\n\n\n\n\n\nSector Influence: All the stocks belong to the semiconductor or technology sector, so most correlations are positive. However, the degree of correlation varies, reflecting differences in product focus (e.g., GPUs, CPUs, or mobile chips) and market exposure.\nPortfolio Implications: Stocks with lower correlations (e.g., AMD and Qualcomm) provide better diversification benefits compared to those with higher correlations (e.g., Texas Instruments and Broadcom). If you aim to reduce portfolio risk, combining less correlated stocks can help mitigate overall volatility.\nIndustry Dynamics: High correlations (e.g., between Texas Instruments and others) suggest these stocks respond similarly to sector-wide factors, such as global semiconductor demand or supply chain disruptions.\n\n\n\n\n\n Multi-GARCH: O-GARCH(1,1) model \nThe O-GARCH model was chosen to estimate the factor structure of the stock returns and univariate GARCH (1,1) model with principal component analysis (PCA). Observing the cumulative proportion of the PCs, we need the 3 PCs to explain at least 85% of the factor variances. Since the autocorrelation suggests that there is serial correlation for almost all stocks in the portfolio, therefore, a few principal components are used to represent the portfolio variation to a very high degree of accuracy. Furthermore, this method is easy to compute the estimates for volatilities and, guarantees positive definiteness of the covariance matrix.\n\n\n\n\n Value at Risk \nThe Value at Risk in Jan 2024: \\[\\text{VaR}_{0.95} = \\$25,94689\\] for the next trading day.\nWith this estimate, the 5% VaR for the long position of $1 million on the stock for the next trading day becomes $25,94689 for the case of choosing \\(n = 21 days\\) in parameter estimation.\nAdvantages of using the empirical quantile method for VaR calculation include:\n\nSimplicity\nUsing no specific distributional assumption.\n\nHowever, the approach has several drawback:\n\nIt assumes that the distribution of the loss (x_t) remains unchanged from the sample period to the prediction period.\nGiven that VaR is concerned mainly with tail probability, this assumption implies that the predicted loss cannot be greater than that of the historical loss. It is definitely not so in practice.\n\n\n\n\n\n Backtesting on risk forecasts \nThe T period is based on the portfolio returns, calculated by using the portfolio weights and the demeaned log returns shown in my in-sample data.\nThe estimated window at least more than 100 days for the in-sample data is chosen selected based on the O-GARCH model and the choice of probability at 5% level.\nThe initial estimation window (WE) was set at 500 days, as it is noted in Chapter 8 of Financial Risk Forecasting by Jon Danielsson, that GARCH models requires more than at least 300 days as an initial estimated window. Then, I estimated the model with the window and produce the VaR forecast estimate at t = WE +1 ending on 31 December 2023. I changed the estimated window by shifting the start and end points by 1, making the sample a “rolling window”.\nThe VaR estimates computed by using parametric O-GARCH method is closer to the log portfolio returns at 5% level across the selected sample period.\nThis image illustrates a backtesting model that compares financial asset returns (shown in red) against the Value at Risk (VaR) estimates (shown in blue) over time. Here’s what the model represents:\n\n Returns (Red Line):  This shows the actual percentage changes (or returns) of the financial asset over time. The spikes and dips represent periods of high volatility or events that significantly impact the asset’s value.\n VaR (Blue Line):  The VaR is a statistical measure used to estimate the potential loss of a portfolio or asset at a given confidence level over a specific period. For example, a 95% VaR of -0.05 means there’s a 95% chance that the loss won’t exceed 5%.\n\nThe backtesting analysis indicates whether the VaR model is reliable for predicting potential losses during normal and extreme market conditions. Adjustments might be needed if the model shows consistent under-performance during stress events.\n\n\n\n\n Caution!!! This is just a small scope of analyzing the volatility in financial data. But it can be explored by more univariate models like ARCH (1,1), GARCH(1,1). \n\n\n\n\n\n\n\n\n Social Media \n\n\n\n\nLinkedIn\n\n\n\n\n\n\n\nMedium: @atomamaro\n\n\n\n\n\n\n\nGithub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVolatility is known as an important measure in finance,\nto understand the risk associated with an asset.\n\n\n\n\n\n\n\n\n\\[\\Sigma_t = \\beta \\Sigma_{f,t} \\beta' + \\Sigma_{\\epsilon,t}\\]\n\n\n\n\n\n\\[r_{t+1} = \\log(1 + R_{t+1}) \\approx R_{t+1}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCited: DALL-E generated image\n\n\nOGARCH = PCA + GARCH \\[\\Sigma_t = \\beta \\Sigma_{f,t} \\beta' + \\Sigma_{\\epsilon,t}\\]\n\n\n\n\\[p = F(-\\text{VaR}_p) = \\int_{-\\infty}^{\\text{VaR}_p} f(x) \\, dx\\]"
  }
]